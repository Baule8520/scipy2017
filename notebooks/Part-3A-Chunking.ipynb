{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3A-Chunking in HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * Explain the concept of data chunking\n",
    "> * Show how to create and read datasets that are chunked\n",
    "> * Learn how to choose reasonable chunk sizes for your datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF5 library supports several layouts so as to store datasets.\n",
    "\n",
    "* Continuous layout:\n",
    "  ![Continuous](img/dset_contiguous4x4.jpg)\n",
    "  More compact, and usually it can be read faster.  Typically used for small datasets (< 1 MB).\n",
    "  \n",
    "* Chunked layout:\n",
    "  ![Chunked](img/dset_chunked4x4.jpg)\n",
    "  Datasets can be enlarged and compressed.  Can be read fast using a fast decompressor. Typically used for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "data_dir = \"chunking\"\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating chunked datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_files(size, chunksize):\n",
    "    data = np.arange(size, dtype=np.int64)\n",
    "\n",
    "    # Contiguous array\n",
    "    with h5py.File(os.path.join(data_dir, \"continuous.h5\"), \"w\") as f:\n",
    "        f.create_dataset(data=data, name=\"data\", dtype=np.int64)\n",
    "\n",
    "    # Simple chunking\n",
    "    with h5py.File(os.path.join(data_dir, \"chunked.h5\"), \"w\") as f:\n",
    "        dset = f.create_dataset(\"data\", (size,), chunks=(chunksize,), dtype=np.int64)\n",
    "        dset[:] = data\n",
    "\n",
    "    # Automatic chunking and unlimited resizing\n",
    "    with h5py.File(os.path.join(data_dir, \"automatic.h5\"), \"w\") as f:\n",
    "        dset = f.create_dataset(\"data\", (0,), chunks=True, maxshape=(None,), dtype=np.int64)\n",
    "        dset.resize((size,))\n",
    "        dset[:] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_files(size=1000, chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"chunking/chunked.h5\" with sec2 driver.\n",
      "data                     Dataset {1000/1000}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Chunks:    {100} 800 bytes\n",
      "    Storage:   8000 logical bytes, 8000 allocated bytes, 100.00% utilization\n",
      "    Type:      native long long\n"
     ]
    }
   ],
   "source": [
    "!h5ls -v {data_dir}/chunked.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"chunking/automatic.h5\" with sec2 driver.\n",
      "data                     Dataset {1000/Inf}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Chunks:    {1024} 8192 bytes\n",
      "    Storage:   8000 logical bytes, 8192 allocated bytes, 97.66% utilization\n",
      "    Type:      native long long\n"
     ]
    }
   ],
   "source": [
    "!h5ls -v {data_dir}/automatic.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is 2ACD-5F91\n",
      "\n",
      " Directory of D:\\SciPy2017\n",
      "\n",
      "\n",
      " Directory of D:\\SciPy2017\\chunking\n",
      "\n",
      "19-06-2017  09:56    <DIR>          .\n",
      "19-06-2017  09:56    <DIR>          ..\n",
      "19-06-2017  09:56            11.688 automatic.h5\n",
      "19-06-2017  09:56            11.496 chunked.h5\n",
      "19-06-2017  09:56            10.144 continuous.h5\n",
      "               3 File(s)         33.328 bytes\n",
      "               2 Dir(s)  13.246.529.536 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls -l chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reading chunked datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading continuous.h5...\n",
      "859 µs ± 62.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "reading chunked.h5...\n",
      "908 µs ± 54 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "reading automatic.h5...\n",
      "903 µs ± 93.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "for h5file in (\"continuous.h5\", \"chunked.h5\", \"automatic.h5\"):\n",
    "    print(\"reading %s...\" % h5file)\n",
    "    %timeit h5py.File(os.path.join(data_dir, h5file))['data'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In the example above, set the `size` to 10 million and choose a minimal `chunksize` that offers a reasonable filesize and read speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 tomkooij 197613 77M Jun 19 09:58 chunking/automatic.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 77M Jun 19 09:58 chunking/chunked.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 77M Jun 19 09:58 chunking/continuous.h5\n"
     ]
    }
   ],
   "source": [
    "create_files(size=1000*1000*10, chunksize=100000)\n",
    "!ls -lh chunking/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading continuous.h5...\n",
      "68 ms ± 5.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "reading chunked.h5...\n",
      "89.3 ms ± 4.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "reading automatic.h5...\n",
      "210 ms ± 17.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "for h5file in (\"continuous.h5\", \"chunked.h5\", \"automatic.h5\"):\n",
    "    print(\"reading %s...\" % h5file)\n",
    "    %timeit h5py.File(os.path.join(data_dir, h5file))['data'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Using the 10 million datasets above, retrieve just a small slice (say [10000:30000]) for each and time the time it takes to read.  Do you think that the whole dataset needs to be read in any case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading continuous.h5...\n",
      "707 µs ± 10.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "reading chunked.h5...\n",
      "720 µs ± 2.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "reading automatic.h5...\n",
      "724 µs ± 4.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "for h5file in (\"continuous.h5\", \"chunked.h5\", \"automatic.h5\"):\n",
    "    print(\"reading %s...\" % h5file)\n",
    "    %timeit h5py.File(os.path.join(data_dir, h5file))['data'][10000:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the right chunksize\n",
    "\n",
    "Chunks are **atomic** objects in the HDF5 library. For each access to an element in a chunk, the entire chunk needs to be read/written\n",
    "\n",
    "Make sure you:\n",
    "- a \n",
    "- b\n",
    "- c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Investigate the difference by reading a dataset row (`dataset[i]`) and by column (`dataset[:, i]`). Remeber the HDF5 library is a C-library: It store data by rows. (Rows are fast, columns are slow).\n",
    "\n",
    "We start of with a contiguous dataset. Try improving this by chosing chunksize.\n",
    "\n",
    "Try automatic chunksize first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"chunksize.h5\")\n",
    "f = h5py.File(FILENAME, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = (10000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('contiguous', shape, dtype=np.int16)\n",
    "dset[:] = np.ones(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 µs ± 13.3 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.95 ms ± 95.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dset[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic chuncksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset = f.create_dataset('auto', shape,  chunks=True, dtype=np.int16)\n",
    "dset[:] = np.ones(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 63)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 µs ± 5.01 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5 ms ± 30 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dset[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset('col_chunks', shape, chunks=(5000, 10), dtype=np.int16)\n",
    "dset[:] = np.ones(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33 ms ± 38.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.68 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dset[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
