{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Using compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * How to compress chunked datasets\n",
    "> * Learn how to fine-tune the HDF5 compression pipeline to suit your needs\n",
    "> * How to use pandas for reading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "data_dir = \"compression\"\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: the movielens-1M datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous work by Greg Reda: http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/\n",
    "\n",
    "This dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. These files contain 1,000,209 anonymous ratings of approximately 3,900 movies  made by 6,040 MovieLens users who joined MovieLens in 2000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import CSV files via pandas\n",
    "dset = 'datasets\\movielens-1m'\n",
    "fdata = os.path.join(dset, 'ratings.dat.gz')\n",
    "fitem = os.path.join(dset, 'movies.dat.gz')\n",
    "\n",
    "# pass in column names for each CSV\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(fdata, sep=';', names=r_cols)\n",
    "\n",
    "m_cols = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_csv(fitem, sep=';', names=m_cols,\n",
    "                     dtype={'title': object, 'genres': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                               title                        genres\n",
       "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4         5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3883 entries, 0 to 3882\n",
      "Data columns (total 3 columns):\n",
      "movie_id    3883 non-null int64\n",
      "title       3883 non-null object\n",
      "genres      3883 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 91.1+ KB\n"
     ]
    }
   ],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_id     int64:dense\n",
       "title       object:dense\n",
       "genres      object:dense\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.ftypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  unix_timestamp\n",
       "0        1      1193       5       978300760\n",
       "1        1       661       3       978302109\n",
       "2        1       914       3       978301968\n",
       "3        1      3408       4       978300275\n",
       "4        1      2355       5       978824291"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id           int64:dense\n",
       "movie_id          int64:dense\n",
       "rating            int64:dense\n",
       "unix_timestamp    int64:dense\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.ftypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing in HDF5/PyTables in compressed form\n",
    "\n",
    "We enable compression by using the HDF5 Filters. These are accessed in `pytables` using the `tables.Filters` class:\n",
    "\n",
    "```\n",
    "filters = tables.Filters(complevel=2, complib='zlib')\n",
    "f.create_table(..., filters=filters)\n",
    "```\n",
    "\n",
    "`table.Filters()` kwargs:\n",
    "\n",
    "* `complevel`\n",
    "    compression level (`complevel`): 0 (no compression) to 9 (maximum compression) \n",
    "\n",
    "* `complib` compression libraries:\n",
    "  - **zlib** (standard HDF5)\n",
    "  - **lzo** (usually not available) \n",
    "  - **bzip2**\n",
    "  - **BLOSC**\n",
    "      - blosc:blosclz\n",
    "      - blosc:lz4\n",
    "      - blosc:lz4hc\n",
    "      - blosc:snappy\n",
    "      - blosc:zlib\n",
    "      - blosc:zstd\n",
    "* `shuffle` use shuffle filter \n",
    "\n",
    "---------\n",
    "\n",
    "`to_hdf5` creates a HDF5 with the Movielens dataset with a filename corresponding to the `Filters` settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_hdf5(ratings, movies, filters):\n",
    "    \n",
    "    class Ratings(tables.IsDescription):\n",
    "        user_id = tables.Int32Col(pos=0)\n",
    "        movie_id = tables.Int32Col(pos=1)\n",
    "        rating = tables.Int8Col(pos=2)\n",
    "        unix_timestamp = tables.Int64Col(pos=3)\n",
    "    \n",
    "    class Movies(tables.IsDescription):\n",
    "        movie_id = tables.Int32Col(pos=0)\n",
    "        title = tables.StringCol(100, pos=1)\n",
    "        genres = tables.StringCol(50, pos=2)\n",
    "    \n",
    "    def get_filename(filters):\n",
    "        if filters.complevel != 0:\n",
    "            complib = filters.complib if \":\" not in filters.complib else filters.complib.replace(\":\", \"-\")\n",
    "            shuffle = \"shuffle\" if filters.shuffle else \"noshuffle\"\n",
    "            filename = \"%s/%s-%d-%s.h5\" % (data_dir, complib, filters.complevel, shuffle)\n",
    "        else:\n",
    "            filename = \"%s/no-compression.h5\" % (data_dir,)\n",
    "        return filename\n",
    "\n",
    "    filename = get_filename(filters)\n",
    "    print(\"Creating file:\", filename)\n",
    "    with tables.open_file(filename, \"w\") as f:\n",
    "        table_ratings = f.create_table(f.root, \"ratings\", Ratings, filters=filters, expectedrows=len(ratings))\n",
    "        table_ratings.append([ratings[col].values for col in ratings.ftypes.keys()])\n",
    "        table_movies = f.create_table(f.root, \"movies\", Movies, filters=filters, expectedrows=len(movies))\n",
    "        table_movies.append([movies[col].values for col in movies.ftypes.keys()])\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file: compression/zlib-5-shuffle.h5\n",
      "Wall time: 621 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filters = tables.Filters(complevel=5)\n",
    "h5file = to_hdf5(ratings, movies, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\n",
      "/movies (Table(3883,), shuffle, zlib(5)) ''\n",
      "  description := {\n",
      "  \"movie_id\": Int32Col(shape=(), dflt=0, pos=0),\n",
      "  \"title\": StringCol(itemsize=100, shape=(), dflt=b'', pos=1),\n",
      "  \"genres\": StringCol(itemsize=50, shape=(), dflt=b'', pos=2)}\n",
      "  byteorder := 'little'\n",
      "  chunkshape := (425,)\n",
      "/ratings (Table(1000209,), shuffle, zlib(5)) ''\n",
      "  description := {\n",
      "  \"user_id\": Int32Col(shape=(), dflt=0, pos=0),\n",
      "  \"movie_id\": Int32Col(shape=(), dflt=0, pos=1),\n",
      "  \"rating\": Int8Col(shape=(), dflt=0, pos=2),\n",
      "  \"unix_timestamp\": Int64Col(shape=(), dflt=0, pos=3)}\n",
      "  byteorder := 'little'\n",
      "  chunkshape := (7710,)\n"
     ]
    }
   ],
   "source": [
    "!ptdump -v {h5file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Compression. Create/write speed.\n",
    "\n",
    "PyTables comes with out-of-box support for a series of codecs.  Do a quick comparison between \"zlib\", \"bzip2\", and \"blosc\" for compression levels of 1 (fastest), 5 and 9 (slowest).  Which one compresses best?  Which one compresses faster?\n",
    "\n",
    "Also, Blosc being a meta-compressor, it has support for different codecs internally that can be selected from PyTables in the \"blosc:`codec`\" form.  Do another comparison between internal Blosc codecs, namely, \"blosc:blosclz\" (the default), \"blosc:lz4\", \"blosc:lz4hc\", \"blosc:snappy\", \"blosc:zlib\" and \"blosc:zstd\".\n",
    "\n",
    "Finally, avoid any compression totally (`complevel=0`).  How fast it is compared with existing codecs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file: compression/zlib-5-shuffle.h5\n",
      "Wall time: 610 ms\n",
      "Creating file: compression/bzip2-5-shuffle.h5\n",
      "Wall time: 1.36 s\n",
      "Creating file: compression/blosc-5-shuffle.h5\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "for complib in (\"zlib\", \"bzip2\", \"blosc\"):\n",
    "    filters = tables.Filters(complevel=5, complib=complib)\n",
    "    %time to_hdf5(ratings, movies, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14M\n",
      "-rw-r--r-- 1 tomkooij 197613 5.0M Jun 26 10:22 blosc-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 4.1M Jun 26 10:22 bzip2-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 4.3M Jun 26 10:22 zlib-5-shuffle.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file: compression/blosc-blosclz-5-shuffle.h5\n",
      "Wall time: 141 ms\n",
      "Creating file: compression/blosc-lz4-5-shuffle.h5\n",
      "Wall time: 94.5 ms\n",
      "Creating file: compression/blosc-lz4hc-5-shuffle.h5\n",
      "Wall time: 280 ms\n",
      "Creating file: compression/blosc-snappy-5-shuffle.h5\n",
      "Wall time: 96.7 ms\n",
      "Creating file: compression/blosc-zlib-5-shuffle.h5\n",
      "Wall time: 560 ms\n",
      "Creating file: compression/blosc-zstd-5-shuffle.h5\n",
      "Wall time: 355 ms\n"
     ]
    }
   ],
   "source": [
    "for complib in (\"blosc:blosclz\", \"blosc:lz4\", \"blosc:lz4hc\", \"blosc:snappy\", \"blosc:zlib\", \"blosc:zstd\"):\n",
    "    filters = tables.Filters(complevel=5, complib=complib)\n",
    "    %time to_hdf5(ratings, movies, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file: compression/no-compression.h5\n",
      "Wall time: 75.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'compression/no-compression.h5'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, the uncompressed case\n",
    "filters = tables.Filters(complevel=0)\n",
    "%time to_hdf5(ratings, movies, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 60M\n",
      "-rw-r--r-- 1 tomkooij 197613 5.0M Jun 26 10:22 blosc-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 5.0M Jun 26 10:22 blosc-blosclz-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 5.4M Jun 26 10:22 blosc-lz4-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 4.8M Jun 26 10:22 blosc-lz4hc-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 5.5M Jun 26 10:22 blosc-snappy-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 4.4M Jun 26 10:22 blosc-zlib-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 4.3M Jun 26 10:22 blosc-zstd-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 4.1M Jun 26 10:22 bzip2-5-shuffle.h5\n",
      "-rw-r--r-- 1 tomkooij 197613  17M Jun 26 10:22 no-compression.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 4.3M Jun 26 10:22 zlib-5-shuffle.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading compressed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = list(os.walk(data_dir))[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blosc-5-shuffle.h5',\n",
       " 'blosc-blosclz-5-shuffle.h5',\n",
       " 'blosc-lz4-5-shuffle.h5',\n",
       " 'blosc-lz4hc-5-shuffle.h5',\n",
       " 'blosc-snappy-5-shuffle.h5',\n",
       " 'blosc-zlib-5-shuffle.h5',\n",
       " 'blosc-zstd-5-shuffle.h5',\n",
       " 'bzip2-5-shuffle.h5',\n",
       " 'no-compression.h5',\n",
       " 'zlib-5-shuffle.h5']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Reading compressed datasets\n",
    "\n",
    "Which codec and compression level can read the fastest?  How it does compare with reading an uncompressed dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: blosc-5-shuffle.h5\n",
      "40.3 ms ± 205 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-blosclz-5-shuffle.h5\n",
      "40.4 ms ± 202 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4-5-shuffle.h5\n",
      "31 ms ± 81.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4hc-5-shuffle.h5\n",
      "31.5 ms ± 217 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-snappy-5-shuffle.h5\n",
      "39.3 ms ± 178 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zlib-5-shuffle.h5\n",
      "106 ms ± 1.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zstd-5-shuffle.h5\n",
      "57.6 ms ± 161 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: bzip2-5-shuffle.h5\n",
      "529 ms ± 8.42 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Reading file: no-compression.h5\n",
      "19.7 ms ± 1.95 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Reading file: zlib-5-shuffle.h5\n",
      "115 ms ± 898 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    print(\"Reading file:\", f)\n",
    "    with tables.open_file(os.path.join(data_dir, f)) as h5f:\n",
    "        %timeit h5f.root.ratings[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: BLOSC multithreading\n",
    "\n",
    "Blosc can use multithreading for compressing/decompressing, although it is disabled by default.  You can enable a multithreaded Blosc in a series of ways, but perhaps the easiest is to set the \"BLOSC_NTHREADS\" environment variable to the desired number of threads (typically the available number of cores in your computer).\n",
    "\n",
    "Execute the cell below and re-do the reading benchmarks and look at how the reading speed vary.  Pay special attention to the difference between the CPU times and wall times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"BLOSC_NTHREADS\"] = \"4\"  # set to any other number you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: blosc-5-shuffle.h5 nthreads= 1\n",
      "44.8 ms ± 3.88 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-5-shuffle.h5 nthreads= 2\n",
      "44.8 ms ± 2.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-5-shuffle.h5 nthreads= 4\n",
      "44.1 ms ± 2.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-blosclz-5-shuffle.h5 nthreads= 1\n",
      "40.5 ms ± 267 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-blosclz-5-shuffle.h5 nthreads= 2\n",
      "43.4 ms ± 554 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-blosclz-5-shuffle.h5 nthreads= 4\n",
      "39.2 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4-5-shuffle.h5 nthreads= 1\n",
      "31.5 ms ± 132 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4-5-shuffle.h5 nthreads= 2\n",
      "34.4 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4-5-shuffle.h5 nthreads= 4\n",
      "34.6 ms ± 130 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4hc-5-shuffle.h5 nthreads= 1\n",
      "31.5 ms ± 56 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4hc-5-shuffle.h5 nthreads= 2\n",
      "31.5 ms ± 126 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-lz4hc-5-shuffle.h5 nthreads= 4\n",
      "33.3 ms ± 226 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-snappy-5-shuffle.h5 nthreads= 1\n",
      "39.6 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-snappy-5-shuffle.h5 nthreads= 2\n",
      "41.1 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-snappy-5-shuffle.h5 nthreads= 4\n",
      "40.2 ms ± 2.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zlib-5-shuffle.h5 nthreads= 1\n",
      "111 ms ± 4.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zlib-5-shuffle.h5 nthreads= 2\n",
      "74.7 ms ± 6.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zlib-5-shuffle.h5 nthreads= 4\n",
      "79.8 ms ± 3.66 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zstd-5-shuffle.h5 nthreads= 1\n",
      "60.6 ms ± 2.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zstd-5-shuffle.h5 nthreads= 2\n",
      "41.8 ms ± 269 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: blosc-zstd-5-shuffle.h5 nthreads= 4\n",
      "50.6 ms ± 561 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Reading file: bzip2-5-shuffle.h5 nthreads= 1\n",
      "533 ms ± 3.37 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Reading file: bzip2-5-shuffle.h5 nthreads= 2\n",
      "526 ms ± 430 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Reading file: bzip2-5-shuffle.h5 nthreads= 4\n",
      "526 ms ± 674 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Reading file: no-compression.h5 nthreads= 1\n",
      "18.8 ms ± 944 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Reading file: no-compression.h5 nthreads= 2\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    for nthreads in [1, 2, 4]:\n",
    "        os.environ[\"BLOSC_NTHREADS\"] = \"%s\" % nthreads\n",
    "        print(\"Reading file:\", f, 'nthreads=', os.environ[\"BLOSC_NTHREADS\"])\n",
    "        with tables.open_file(os.path.join(data_dir, f)) as h5f:\n",
    "            %timeit h5f.root.ratings[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and denormalizing tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many data sources are expressed in terms of related tables.  For example, part of the [MovieLens dataset](https://grouplens.org/datasets/movielens/) is structured in tables having the next columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "movies = ['movie_id', 'title', 'genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation that links the two tables above is the `movie_id` field.  This way, one can query parts of the dataset that involve the two tables, like for example, which users ('user_id') gave a rating of 5 to some movie ('title').  This is called the `normalized` version and we have already dealt with that in a previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, one can fuse the above 2 tables into a single one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings_movies = ['title', 'genres', 'user_id', 'rating', 'unix_timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, we still keep all the data fields, except for the 'movie_id' that is not needed anymore.  This is called the `denormalized` version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this one is that we have all the fields readily available in one single table, so querying it and getting info about all the fileds is straighforward.  The disadvantage is that this table will have many duplicated information, i.e. the 'title' and 'genres' fields will appear for all the ratings, which can be seen as a waste of space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, many times compression can get rid of many of the duplicated info in denormalized tables.  Let's see how to produce a denormalized table and how it fares compared with the normalized version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Denormalizing tables using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import CSV files via pandas\n",
    "dset = 'datasets\\movielens-1m'\n",
    "fdata = os.path.join(dset, 'ratings.dat.gz')\n",
    "fitem = os.path.join(dset, 'movies.dat.gz')\n",
    "\n",
    "# pass in column names for each CSV\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(fdata, sep=';', names=r_cols)\n",
    "\n",
    "m_cols = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_csv(fitem, sep=';', names=m_cols,\n",
    "                     dtype={'title': object, 'genres': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create one merged DataFrame\n",
    "lens = pd.merge(movies, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.ftypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_hdf5_denorm(lens, filters):\n",
    "\n",
    "    class Lens(tables.IsDescription):\n",
    "        user_id = tables.Int32Col(pos=0)\n",
    "        rating = tables.Int8Col(pos=1)\n",
    "        unix_timestamp = tables.Int64Col(pos=2)\n",
    "        title = tables.StringCol(100, pos=3)\n",
    "        genres = tables.StringCol(50, pos=4)\n",
    "        \n",
    "    def get_filename(filters):\n",
    "        if filters.complevel != 0:\n",
    "            complib = filters.complib if \":\" not in filters.complib else filters.complib.replace(\":\", \"-\")\n",
    "            shuffle = \"shuffle\" if filters.shuffle else \"noshuffle\"\n",
    "            filename = \"%s/%s-%d-%s-denorm.h5\" % (data_dir, complib, filters.complevel, shuffle)\n",
    "        else:\n",
    "            filename = \"%s/no-compression-denorm.h5\" % (data_dir,)\n",
    "        return filename\n",
    "\n",
    "    filename = get_filename(filters)\n",
    "    print(\"Creating file:\", filename)\n",
    "    with tables.open_file(filename, \"w\", filters=filters) as f:\n",
    "        table_lens = f.create_table(f.root, \"lens\", Lens)\n",
    "        table_lens.append([lens[col].values for col in table_lens.dtype.names])\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filters = tables.Filters(complevel=0)\n",
    "h5file = to_hdf5_denorm(lens, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ptdump -v -R0,10 {h5file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the size of the denormalized table is much larger than the normalized one (156 MB vs 17 MB).  But that is without using compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Compressing a denormalized table\n",
    "\n",
    "Create a compressed version of the denormalized table and compare it with the same table in the normalized state.\n",
    "What's the difference in size now?  Why do you think the compression process works much better in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = tables.Filters(complevel=5, complib=\"blosc:blosclz\")\n",
    "%time to_hdf5_denorm(lens, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Denormalized table: compare codecs.\n",
    "\n",
    "Create different files containing the denormalized table using different codecs.  Which one reduces the size better?  How does it compare with the files for the normalized version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for complib in (\"zlib\", \"bzip2\", \"blosc:blosclz\", \"blosc:lz4\", \"blosc:lz4hc\", \"blosc:snappy\", \"blosc:zlib\", \"blosc:zstd\"):\n",
    "    filters = tables.Filters(complevel=5, complib=complib)\n",
    "    %time to_hdf5_denorm(lens, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will see the effect of querying normalized and denormalized tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make sure datafiles for next notebook exist:\n",
    "\n",
    "We will reuse some hdf5 files in the next notebook, make sure they exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"compression/no-compression-denorm.h5\"):\n",
    "    filters = tables.Filters(complevel=0)\n",
    "    h5file = to_hdf5_denorm(lens, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"compression/blosc-zstd-5-shuffle-denorm.h5\"):\n",
    "    filters = tables.Filters(complevel=5, complib='blosc:zstd')\n",
    "    h5file = to_hdf5_denorm(lens, filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"compression/blosc-zstd-5-shuffle-denorm.h5\"):\n",
    "    filters = tables.Filters(complevel=5, complib='blosc:zstd')\n",
    "    h5file = to_hdf5(ratings, movies, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hdf5]",
   "language": "python",
   "name": "conda-env-hdf5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
