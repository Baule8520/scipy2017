{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Datatypes in HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * How to create (and read) HDF5 files with datasets of homogeneous, heterogenous and nested datatypes\n",
    "> * See how h5py and PyTables achieves the same thing with their own APIs\n",
    "> * Be introduced to the `IsDescription` class in PyTables for declaring tables (instead of NumPy dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "data_dir = \"datatypes\"\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneous datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_to_store = np.arange(10, dtype=np.int8)\n",
    "arr_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"homogenous_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `h5py.File` object supports both the `create_dataset` method and a `dict` like access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"mydata\": shape (10,), type \"|i1\">"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.create_dataset(data=arr_to_store, name=\"mydata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['/mydata2'] = arr_to_store    # data can be accessed in a NumPy-like interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mydata', 'mydata2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset with `[:]` or `[...]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['/mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF5 library provides `h5ls` and ``h5dump` to investigate the contents of HDF5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {10}\n",
      "mydata2                  Dataset {10}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"datatypes\\homogenous_h5py.h5\" with sec2 driver.\n",
      "/                        Group\n",
      "    Location:  1:96\n",
      "    Links:     1\n",
      "/mydata                  Dataset {10/10}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\n",
      "    Type:      native signed char\n",
      "/mydata2                 Dataset {10/10}\n",
      "    Location:  1:1400\n",
      "    Links:     1\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\n",
      "    Type:      native signed char\n"
     ]
    }
   ],
   "source": [
    "!h5ls -rv {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 \"datatypes\\homogenous_h5py.h5\" {\n",
      "GROUP \"/\" {\n",
      "   DATASET \"mydata\" {\n",
      "      DATATYPE  H5T_STD_I8LE\n",
      "      DATASPACE  SIMPLE { ( 10 ) / ( 10 ) }\n",
      "      DATA {\n",
      "      (0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "      }\n",
      "   }\n",
      "   DATASET \"mydata2\" {\n",
      "      DATATYPE  H5T_STD_I8LE\n",
      "      DATASPACE  SIMPLE { ( 10 ) / ( 10 ) }\n",
      "      DATA {\n",
      "      (0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "      }\n",
      "   }\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!h5dump {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 23 10:35 homogenous_h5py.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"homogenous_pytables.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PyTables` datasets are wrapped into high levels objects:\n",
    "* **Array**: homogenous dataset\n",
    "* **CArray**: homogenous dataset, chunked storage (more on this later)\n",
    "* **EArray**: homogenous dataset, extendable. Supports `.append()`.\n",
    "* **Table**: compound dataset, extendable. Supports `.append()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Array(10,)) ''\n",
       "  atom := Int8Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.create_array(f2.root, name=\"mydata\", obj=arr_to_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a dataset into memory is similar to `h5py` with `[:]` or `[...]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.root.mydata[:]  # data can be accessed in a NumPy-like interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTables` also provides a `.read()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.root.mydata.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File(filename=datatypes\\homogenous_pytables.h5, title='', mode='w', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) ''\n",
       "/mydata (Array(10,)) ''\n",
       "  atom := Int8Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\n",
      "/mydata                  Dataset {10}\n"
     ]
    }
   ],
   "source": [
    "!h5ls -r {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 23 10:35 homogenous_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 23 10:35 homogenous_pytables.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compound Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = np.dtype([(\"myfield1\", np.int32), (\"myfield2\", np.float64), (\"myfield3\", \"S5\")])\n",
    "table_to_store = np.fromiter(((i, i**2, \"foo_%d\"%i) for i in range(10)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['mydata'] = table_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"mydata\": shape (10,), type \"|V17\">"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"datatypes\\compound_h5py.h5\" with sec2 driver.\n",
      "mydata                   Dataset {10/10}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Storage:   170 logical bytes, 170 allocated bytes, 100.00% utilization\n",
      "    Type:      struct {\n",
      "                   \"myfield1\"         +0    native int\n",
      "                   \"myfield2\"         +4    native double\n",
      "                   \"myfield3\"         +12   5-byte null-padded ASCII string\n",
      "               } 17 bytes\n"
     ]
    }
   ],
   "source": [
    "!h5ls -v {FILENAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `datatypes\\compound_h5py.h5` in PyTables and investigate the dataset.\n",
    "\n",
    "Look at the dataset description. Read it from disk. Look at the dtype.\n",
    "\n",
    "*Optional: Create a new table. Can you append some data?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_h5py.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\n",
      "/mydata (Table(10,)) ''\n"
     ]
    }
   ],
   "source": [
    "!ptdump {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE!!!\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileh = tables.open_file(FILENAME, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File(filename=datatypes\\compound_h5py.h5, title='', mode='r', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) ''\n",
       "/mydata (Table(10,)) ''\n",
       "  description := {\n",
       "  \"myfield1\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"myfield2\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"myfield3\": StringCol(itemsize=5, shape=(), dflt=b'', pos=2)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3855,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = fileh.root.mydata[:]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables (using numpy.dtype)\n",
    "\n",
    "In PyTables a compound dataset is called a `Table`.\n",
    "\n",
    "To store a table we use `create_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_pytables1.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(0,)) ''\n",
       "  description := {\n",
       "  \"myfield1\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"myfield2\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"myfield3\": StringCol(itemsize=5, shape=(), dflt=b'', pos=2)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3855,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = f2.create_table(f2.root, name=\"mydata\", description=table_to_store.dtype)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Table` class has high level functions, such as `append()` and `remove_row()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.append(table_to_store)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.remove_row(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (6,  36., b'foo_6'),\n",
       "       (7,  49., b'foo_7'), (8,  64., b'foo_8'), (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,  1., b'foo_1')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables (using tables.description)\n",
    "\n",
    "In PyTables it is convenient to define compound datasets using the `tables.IsDescription` class, instead of (complicated) numpy dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyTable(tables.IsDescription):\n",
    "    myfield1 = tables.Int32Col()\n",
    "    myfield2 = tables.Float64Col()\n",
    "    myfield3 = tables.StringCol(itemsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_pytables2.h5\")\n",
    "f3 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = f3.create_table(f3.root, \"mydata\", MyTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.append(table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the HDF5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 156\n",
      "-rw-r--r-- 1 tomkooij 197613  2314 Jun 23 10:35 compound_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 69879 Jun 23 10:35 compound_pytables1.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 69879 Jun 23 10:35 compound_pytables2.h5\n",
      "-rw-r--r-- 1 tomkooij 197613  2164 Jun 23 10:35 homogenous_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613  2154 Jun 23 10:35 homogenous_pytables.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -l {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it seems like PyTables files are larger than h5py ones, why?  Let's introspect a bit into the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {10}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {data_dir}/compound_h5py.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {9/Inf}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {data_dir}/compound_pytables1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dimensionality of the table created with PyTables is `{10/Inf}` (or `{9/Inf}` if we deleted a row), indicating that the dataset is chunked, whereas the one created with h5py is just `{10}`, which means that it is not using chunking.  As chunked datasets take more space than non-chunked ones, this is why PyTables are larger.\n",
    "\n",
    "The reason why PyTables tables are chunked by default is that they can be enlarged and compressed, and chunking is required in order to allow that.  More on chunking later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nested fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PyTables` API provides some useful tools to handle nested columns in compound datasets (tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NestedTable(tables.IsDescription):\n",
    "    \"\"\"A nested table\"\"\"\n",
    "    name = tables.StringCol(10, pos=0)\n",
    "    \n",
    "    class momentum(tables.IsDescription):\n",
    "        p_x = tables.Float64Col()\n",
    "        p_y = tables.Float64Col()\n",
    "        p_z = tables.Float64Col() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"nested.h5\")\n",
    "f4 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = f4.create_table(f4.root, \"mydata\", NestedTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(0,)) ''\n",
       "  description := {\n",
       "  \"name\": StringCol(itemsize=10, shape=(), dflt=b'', pos=0),\n",
       "  \"momentum\": {\n",
       "    \"p_x\": Float64Col(shape=(), dflt=0.0, pos=0),\n",
       "    \"p_y\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "    \"p_z\": Float64Col(shape=(), dflt=0.0, pos=2)}}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1927,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = t.dtype\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'foo_0', ( 0.,  10.,  20.)), (b'foo_1', ( 1.,  11.,  21.)),\n",
       "       (b'foo_2', ( 2.,  12.,  22.)), (b'foo_3', ( 3.,  13.,  23.)),\n",
       "       (b'foo_4', ( 4.,  14.,  24.)), (b'foo_5', ( 5.,  15.,  25.)),\n",
       "       (b'foo_6', ( 6.,  16.,  26.)), (b'foo_7', ( 7.,  17.,  27.)),\n",
       "       (b'foo_8', ( 8.,  18.,  28.)), (b'foo_9', ( 9.,  19.,  29.))],\n",
       "      dtype=[('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store = np.fromiter(((\"foo_%s\"%i, (i, 10+i, 20+i)) for i in range(10)), dtype=dtype)\n",
    "table_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the `p_x` nested column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store['momentum']['p_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.append(table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `Cols` accessor. (PyTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.col(name)` reads the entire column.\n",
    "\n",
    "`table.col('momentum')` will read the entire column (array) in memory and slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.col('momentum')[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `cols` accessor, we can access the column without reading the entire column in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cols.momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cols.momentum[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested columns can be accessed by the `Cols` accessor using natural naming: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cols.momentum.p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Investigate reading a small part of a large table from disk.\n",
    "\n",
    " * Store the table in a HDF5 file. (Using either PyTables or h5py).\n",
    " * Read elements [20:30] from the p_x column.\n",
    "\n",
    "Is the entire datafile being read from disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NestedTable(tables.IsDescription):\n",
    "    \"\"\"A nested table\"\"\"\n",
    "    i = tables.Int32Col()\n",
    "    \n",
    "    class momentum(tables.IsDescription):\n",
    "        p_x = tables.Float64Col()\n",
    "        p_y = tables.Float64Col()\n",
    "        p_z = tables.Float64Col() \n",
    "        \n",
    "dtype = tables.description.dtype_from_descr(NestedTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"large\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = int(1e6)\n",
    "table_to_store = np.fromiter(((i, (i, i, i)) for i in range(N)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the IPython magic `%time` or `%timeit` to time the reading of data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(data_dir, 'big.h5')\n",
    "f = tables.open_file(fn, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.create_table('/', 'mydata', table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f.root.mydata[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f.root.mydata.cols.momentum.p_x[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `h5py` there is no equivalent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(fn, 'a')\n",
    "dset = f['mydata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dset['momentum']['p_x'][20:30]  # this reads the entire nested columns and selects `p_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dset['momentum']['p_x'][20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Using the hierarchy and compound datasets (tables).\n",
    "\n",
    "`ufo-scrubbed.csv` is a (scrubbed) partial dataset of UFO Sightings from across the world.\n",
    "\n",
    "Store the UFO sightings in HDF5. Assume the real (full) dataset is VERY large. Store the data in multiple tables (geospatial).\n",
    "Make sure you use correct dtype etc.\n",
    "\n",
    "Use h5py and/or pytables.\n",
    "\n",
    "**The objective of this excercise is to get hands-on experience with handling compound datatypes, groups, dtypes etc.**\n",
    "\n",
    "-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, do not use `pandas` in this exercise (we will use it later on). But let's use pandas for a quick overview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/ufo_scrubbed.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset CSV and create a dict of sighting per country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('datasets/ufo_scrubbed.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    dataset = [tuple(line) for line in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictonary of sightings per country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "sightings = defaultdict(list)\n",
    "\n",
    "for sighting in dataset[1:]:\n",
    "    dt, city, state, country, _, duration, _, comments, _, lat, lon = sighting\n",
    "    sightings[country].append((dt, city, state, duration, lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sightings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sightings['de'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list in the dictonary is a `numpy.recarray` like object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sightings['de'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually set the `dtype` (or use `tables.IsDescription`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype=np.dtype([('datetime', 'S16'), ('city', 'S20'), ('state', 'S2'), ('duration', np.float32), ('lat', 'f8'), ('lon', 'f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sightings['us'], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(data_dir, 'ufo.h5')\n",
    "f = h5py.File(fn, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for country in sightings.keys():\n",
    "    if country=='':\n",
    "        group_name = 'other'\n",
    "    else:\n",
    "        group_name = country\n",
    "    f.create_group(group_name)\n",
    "    f.create_dataset('%s/data' % group_name, data=np.array(sightings[country], dtype=dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls -r {fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PyTables` datasets are wrapped into high levels objects:\n",
    "* **Array**: homogenous dataset\n",
    "* **CArray**: homogenous dataset, chunked storage (more on this later)\n",
    "* **EArray**: homogenous dataset, extendable. Supports `.append()`.\n",
    "* **Table**: compound dataset, extendable. Supports `.append()`.\n",
    "\n",
    "1) Create a hdf5 file in `PyTables` with each of the above for dataset types. Open it in `h5py` (and/or use `h5ls` etc). Note the equivalent `h5py` datatype.\n",
    "\n",
    "2) Create the same file in `h5py`, make sure the datasets end up as the above four `PyTables` dataset types. Use `ptdump` to view the file in `PyTables` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(data_dir, 'types.h5')\n",
    "with h5py.File(fn, 'w') as f:\n",
    "    # Array\n",
    "    f['/array'] = np.arange(10)\n",
    "    # CArray\n",
    "    dset = f.create_dataset('/carray', (10,), chunks=True)\n",
    "    dset[:] = np.arange(10)\n",
    "    # EArray\n",
    "    dset = f.create_dataset('/earray', (10,), maxshape=(None,))\n",
    "    dset[:] = np.arange(10)\n",
    "    dset.resize(20, 0)\n",
    "    dset[10:] = np.arange(10)\n",
    "    # Table\n",
    "    dtype=np.dtype([('c1', 'i4'), ('c2','f4')])\n",
    "    table = np.array([(1, 2.5), (3, 6.7)], dtype=dtype)\n",
    "    f['/table'] = table             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ptdump {fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls {fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(data_dir, 'types2.h5')\n",
    "with tables.open_file(fn, 'w') as f:\n",
    "    data = np.arange(10)\n",
    "    f.create_array('/', 'array', obj=data)\n",
    "    f.create_carray('/', 'carray', obj=data)\n",
    "    f.create_earray('/', 'earray', obj=data, expectedrows=10)\n",
    "     # Table\n",
    "    dtype=np.dtype([('c1', 'i4'), ('c2','f4')])\n",
    "    table = np.array([(1, 2.5), (3, 6.7)], dtype=dtype)\n",
    "    f.create_table('/', 'table', obj=table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls {fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hdf5]",
   "language": "python",
   "name": "conda-env-hdf5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
