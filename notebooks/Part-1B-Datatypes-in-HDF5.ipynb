{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1B: Datatypes in HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * How to create (and read) HDF5 files with datasets of homogeneous, heterogenous and nested datatypes\n",
    "> * See how h5py and PyTables achieves the same thing with their own APIs\n",
    "> * Be introduced to the `IsDescription` class in PyTables for declaring tables (instead of NumPy dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "data_dir = \"datatypes\"\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneous datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_to_store = np.arange(10, dtype=np.int8)\n",
    "arr_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"homogenous_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `h5py.File` object supports both the `create_dataset` method and a `dict` like access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (Name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c1e0ba703bab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr_to_store\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mydata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (C:\\bld\\h5py_1497042114229\\work\\h5py-2.7.0\\h5py\\_objects.c:2859)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (C:\\bld\\h5py_1497042114229\\work\\h5py-2.7.0\\h5py\\_objects.c:2817)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, name, obj)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             \u001b[0mh5o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (C:\\bld\\h5py_1497042114229\\work\\h5py-2.7.0\\h5py\\_objects.c:2859)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (C:\\bld\\h5py_1497042114229\\work\\h5py-2.7.0\\h5py\\_objects.c:2817)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link (C:\\bld\\h5py_1497042114229\\work\\h5py-2.7.0\\h5py\\h5o.c:3914)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to create link (Name already exists)"
     ]
    }
   ],
   "source": [
    "f.create_dataset(data=arr_to_store, name=\"mydata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['/mydata2'] = arr_to_store    # data can be accessed in a NumPy-like interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mydata', 'mydata2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset with `[:]` or `[...]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['/mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF5 library provides `h5ls` and ``h5dump` to investigate the contents of HDF5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {10}\n",
      "mydata2                  Dataset {10}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"datatypes\\homogenous_h5py.h5\" with sec2 driver.\n",
      "/                        Group\n",
      "    Location:  1:96\n",
      "    Links:     1\n",
      "/mydata                  Dataset {10/10}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\n",
      "    Type:      native signed char\n",
      "/mydata2                 Dataset {10/10}\n",
      "    Location:  1:1672\n",
      "    Links:     1\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\n",
      "    Type:      native signed char\n"
     ]
    }
   ],
   "source": [
    "!h5ls -rv {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 \"datatypes\\homogenous_h5py.h5\" {\n",
      "GROUP \"/\" {\n",
      "   DATASET \"mydata\" {\n",
      "      DATATYPE  H5T_STD_I8LE\n",
      "      DATASPACE  SIMPLE { ( 10 ) / ( 10 ) }\n",
      "      DATA {\n",
      "      (0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "      }\n",
      "   }\n",
      "   DATASET \"mydata2\" {\n",
      "      DATATYPE  H5T_STD_I8LE\n",
      "      DATASPACE  SIMPLE { ( 10 ) / ( 10 ) }\n",
      "      DATA {\n",
      "      (0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "      }\n",
      "   }\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!h5dump {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 22 08:59 homogenous_h5py.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"homogenous_pytables.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PyTables` datasets are wrapped into high levels objects:\n",
    "* **Array**: homogenous dataset\n",
    "* **CArray**: homogenous dataset, chunked storage (more on this later)\n",
    "* **EArray**: homogenous dataset, extendable. Supports `.append()`.\n",
    "* **Table**: compound dataset, extendable. Supports `.append()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NodeError",
     "evalue": "group ``/`` already has a child node named ``mydata``",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNodeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2571a0bd7a9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mydata\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr_to_store\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36mcreate_array\u001b[1;34m(self, where, name, obj, title, byteorder, createparents, atom, shape)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[0mparentnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_or_create_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreateparents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         return Array(parentnode, name,\n\u001b[1;32m-> 1146\u001b[1;33m                      obj=obj, title=title, byteorder=byteorder)\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\array.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parentnode, name, obj, title, byteorder, _log, _atom)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;31m# Ordinary arrays have no filters: leaf is created with default ones.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         super(Array, self).__init__(parentnode, name, new, Filters(),\n\u001b[1;32m--> 187\u001b[1;33m                                     byteorder, _log)\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_g_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\leaf.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parentnode, name, new, filters, byteorder, _log)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;31m# is a lazy property that automatically handles their loading.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLeaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparentnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\node.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parentnode, name, _log)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;31m# Only new nodes need to be referenced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;31m# Opened nodes are already known by their parent group.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mparentnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_refnode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_set_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparentnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\group.py\u001b[0m in \u001b[0;36m_g_refnode\u001b[1;34m(self, childnode, childname, validate)\u001b[0m\n\u001b[0;32m    514\u001b[0m             raise NodeError(\n\u001b[0;32m    515\u001b[0m                 \u001b[1;34m\"group ``%s`` already has a child node named ``%s``\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m                 % (self._v_pathname, childname))\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;31m# Show a warning if there is an object attribute with that name.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNodeError\u001b[0m: group ``/`` already has a child node named ``mydata``"
     ]
    }
   ],
   "source": [
    "f2.create_array(f2.root, name=\"mydata\", obj=arr_to_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a dataset into memory is similar to `h5py` with `[:]` or `[...]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.root.mydata[:]  # data can be accessed in a NumPy-like interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTables` also provides a `.read()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.root.mydata.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File(filename=datatypes\\homogenous_pytables.h5, title='', mode='w', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) ''\n",
       "/mydata (Array(10,)) ''\n",
       "  atom := Int8Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\n",
      "/mydata                  Dataset {10}\n"
     ]
    }
   ],
   "source": [
    "!h5ls -r {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 22 08:59 homogenous_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 22 09:05 homogenous_pytables.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compound Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = np.dtype([(\"myfield1\", np.int32), (\"myfield2\", np.float64), (\"myfield3\", \"S5\")])\n",
    "table_to_store = np.fromiter(((i, i**2, \"foo_%d\"%i) for i in range(10)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['mydata'] = table_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"mydata\": shape (10,), type \"|V17\">"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"datatypes\\compound_h5py.h5\" with sec2 driver.\n",
      "mydata                   Dataset {10/10}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Storage:   170 logical bytes, 170 allocated bytes, 100.00% utilization\n",
      "    Type:      struct {\n",
      "                   \"myfield1\"         +0    native int\n",
      "                   \"myfield2\"         +4    native double\n",
      "                   \"myfield3\"         +12   5-byte null-padded ASCII string\n",
      "               } 17 bytes\n"
     ]
    }
   ],
   "source": [
    "!h5ls -v {FILENAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `datatypes\\compound_h5py.h5` in PyTables and investigate the dataset.\n",
    "\n",
    "Look at the dataset description. Read it from disk. Look at the dtype.\n",
    "\n",
    "*Optional: Create a new table. Can you append some data?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_h5py.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\n",
      "/mydata (Table(10,)) ''\n"
     ]
    }
   ],
   "source": [
    "!ptdump {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE!!!\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileh = tables.open_file(FILENAME, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File(filename=datatypes\\compound_h5py.h5, title='', mode='r', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) ''\n",
       "/mydata (Table(10,)) ''\n",
       "  description := {\n",
       "  \"myfield1\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"myfield2\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"myfield3\": StringCol(itemsize=5, shape=(), dflt=b'', pos=2)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3855,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = fileh.root.mydata[:]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables (using numpy.dtype)\n",
    "\n",
    "In PyTables a compound dataset is called a `Table`.\n",
    "\n",
    "To store a table we use `create_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_pytables1.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(0,)) ''\n",
       "  description := {\n",
       "  \"myfield1\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"myfield2\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"myfield3\": StringCol(itemsize=5, shape=(), dflt=b'', pos=2)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3855,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = f2.create_table(f2.root, name=\"mydata\", description=table_to_store.dtype)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Table` class has high level functions, such as `append()` and `remove_row()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.append(table_to_store)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (5,  25., b'foo_5'),\n",
       "       (6,  36., b'foo_6'), (7,  49., b'foo_7'), (8,  64., b'foo_8'),\n",
       "       (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.remove_row(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,   0., b'foo_0'), (1,   1., b'foo_1'), (2,   4., b'foo_2'),\n",
       "       (3,   9., b'foo_3'), (4,  16., b'foo_4'), (6,  36., b'foo_6'),\n",
       "       (7,  49., b'foo_7'), (8,  64., b'foo_8'), (9,  81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,  1., b'foo_1')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables (using tables.description)\n",
    "\n",
    "In PyTables it is convenient to define compound datasets using the `tables.IsDescription` class, instead of (complicated) numpy dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyTable(tables.IsDescription):\n",
    "    myfield1 = tables.Int32Col()\n",
    "    myfield2 = tables.Float64Col()\n",
    "    myfield3 = tables.StringCol(itemsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_pytables2.h5\")\n",
    "f3 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = f3.create_table(f3.root, \"mydata\", MyTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.append(table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the HDF5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 156\n",
      "-rw-r--r-- 1 tomkooij 197613  2314 Jun 22 09:06 compound_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 69879 Jun 22 09:09 compound_pytables1.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 69879 Jun 22 09:09 compound_pytables2.h5\n",
      "-rw-r--r-- 1 tomkooij 197613  2174 Jun 22 08:59 homogenous_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613  2154 Jun 22 09:05 homogenous_pytables.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -l {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it seems like PyTables files are larger than h5py ones, why?  Let's introspect a bit into the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {10}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {data_dir}/compound_h5py.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {9/Inf}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {data_dir}/compound_pytables1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dimensionality of the table created with PyTables is `{10/Inf}` (or `{9/Inf}` if we deleted a row), indicating that the dataset is chunked, whereas the one created with h5py is just `{10}`, which means that it is not using chunking.  As chunked datasets take more space than non-chunked ones, this is why PyTables are larger.\n",
    "\n",
    "The reason why PyTables tables are chunked by default is that they can be enlarged and compressed, and chunking is required in order to allow that.  More on chunking later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nested fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PyTables` API provides some useful tools to handle nested columns in compound datasets (tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NestedTable(tables.IsDescription):\n",
    "    \"\"\"A nested table\"\"\"\n",
    "    name = tables.StringCol(10, pos=0)\n",
    "    \n",
    "    class momentum(tables.IsDescription):\n",
    "        p_x = tables.Float64Col()\n",
    "        p_y = tables.Float64Col()\n",
    "        p_z = tables.Float64Col() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The file 'datatypes\\nested.h5' is already opened.  Please close it before reopening in write mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-0e8d89d8bc73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mFILENAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"nested.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILENAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36mopen_file\u001b[1;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 raise ValueError(\n\u001b[0;32m    316\u001b[0m                     \u001b[1;34m\"The file '%s' is already opened.  Please \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                     \"close it before reopening in write mode.\" % filename)\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# Finally, create the File instance, and return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The file 'datatypes\\nested.h5' is already opened.  Please close it before reopening in write mode."
     ]
    }
   ],
   "source": [
    "FILENAME = os.path.join(data_dir, \"nested.h5\")\n",
    "f4 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NodeError",
     "evalue": "group ``/`` already has a child node named ``mydata``",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNodeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-207fa96429a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mydata\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNestedTable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36mcreate_table\u001b[1;34m(self, where, name, description, title, filters, expectedrows, chunkshape, byteorder, createparents, obj)\u001b[0m\n\u001b[0;32m   1053\u001b[0m                       \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                       \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpectedrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpectedrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1055\u001b[1;33m                       chunkshape=chunkshape, byteorder=byteorder)\n\u001b[0m\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\table.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parentnode, name, description, title, filters, expectedrows, chunkshape, byteorder, _log)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m         super(Table, self).__init__(parentnode, name, new, filters,\n\u001b[1;32m--> 833\u001b[1;33m                                     byteorder, _log)\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_g_post_init_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\leaf.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parentnode, name, new, filters, byteorder, _log)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;31m# is a lazy property that automatically handles their loading.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLeaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparentnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\node.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parentnode, name, _log)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;31m# Only new nodes need to be referenced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;31m# Opened nodes are already known by their parent group.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mparentnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_refnode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_set_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparentnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\hdf5\\lib\\site-packages\\tables\\group.py\u001b[0m in \u001b[0;36m_g_refnode\u001b[1;34m(self, childnode, childname, validate)\u001b[0m\n\u001b[0;32m    514\u001b[0m             raise NodeError(\n\u001b[0;32m    515\u001b[0m                 \u001b[1;34m\"group ``%s`` already has a child node named ``%s``\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m                 % (self._v_pathname, childname))\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;31m# Show a warning if there is an object attribute with that name.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNodeError\u001b[0m: group ``/`` already has a child node named ``mydata``"
     ]
    }
   ],
   "source": [
    "t = f4.create_table(f4.root, \"mydata\", NestedTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(0,)) ''\n",
       "  description := {\n",
       "  \"name\": StringCol(itemsize=10, shape=(), dflt=b'', pos=0),\n",
       "  \"momentum\": {\n",
       "    \"p_x\": Float64Col(shape=(), dflt=0.0, pos=0),\n",
       "    \"p_y\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "    \"p_z\": Float64Col(shape=(), dflt=0.0, pos=2)}}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1927,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = t.dtype\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'foo_0', ( 0.,  10.,  20.)), (b'foo_1', ( 1.,  11.,  21.)),\n",
       "       (b'foo_2', ( 2.,  12.,  22.)), (b'foo_3', ( 3.,  13.,  23.)),\n",
       "       (b'foo_4', ( 4.,  14.,  24.)), (b'foo_5', ( 5.,  15.,  25.)),\n",
       "       (b'foo_6', ( 6.,  16.,  26.)), (b'foo_7', ( 7.,  17.,  27.)),\n",
       "       (b'foo_8', ( 8.,  18.,  28.)), (b'foo_9', ( 9.,  19.,  29.))],\n",
       "      dtype=[('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store = np.fromiter(((\"foo_%s\"%i, (i, 10+i, 20+i)) for i in range(10)), dtype=dtype)\n",
    "table_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the `p_x` nested column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store['momentum']['p_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.append(table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'foo_0', ( 0.,  10.,  20.)), (b'foo_1', ( 1.,  11.,  21.)),\n",
       "       (b'foo_2', ( 2.,  12.,  22.)), (b'foo_3', ( 3.,  13.,  23.)),\n",
       "       (b'foo_4', ( 4.,  14.,  24.)), (b'foo_5', ( 5.,  15.,  25.)),\n",
       "       (b'foo_6', ( 6.,  16.,  26.)), (b'foo_7', ( 7.,  17.,  27.)),\n",
       "       (b'foo_8', ( 8.,  18.,  28.)), (b'foo_9', ( 9.,  19.,  29.))],\n",
       "      dtype=[('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `Cols` accessor. (PyTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.col(name)` reads the entire column.\n",
    "\n",
    "`table.col('momentum')` will read the entire column (array) in memory and slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([( 2.,  12.,  22.), ( 3.,  13.,  23.), ( 4.,  14.,  24.)],\n",
       "      dtype=[('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.col('momentum')[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `cols` accessor, we can access the column without reading the entire column in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata.cols.momentum (Cols), 3 columns\n",
       "  p_x (Column(10,), float64)\n",
       "  p_y (Column(10,), float64)\n",
       "  p_z (Column(10,), float64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cols.momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([( 2.,  12.,  22.), ( 3.,  13.,  23.), ( 4.,  14.,  24.)],\n",
       "      dtype=[('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cols.momentum[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested columns can be accessed by the `Cols` accessor using natural naming: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata.cols.momentum.p_x (Column(10,), float64, idx=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cols.momentum.p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Investigate reading a small part of a large table from disk.\n",
    "\n",
    " * Store the table in a HDF5 file. (Using either PyTables or h5py).\n",
    " * Read elements [20:30] from the p_x column.\n",
    "\n",
    "Is the entire datafile being read from disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NestedTable(tables.IsDescription):\n",
    "    \"\"\"A nested table\"\"\"\n",
    "    i = tables.Int32Col()\n",
    "    \n",
    "    class momentum(tables.IsDescription):\n",
    "        p_x = tables.Float64Col()\n",
    "        p_y = tables.Float64Col()\n",
    "        p_z = tables.Float64Col() \n",
    "        \n",
    "dtype = tables.description.dtype_from_descr(NestedTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"large\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = int(1e6)\n",
    "table_to_store = np.fromiter(((i, (i, i, i)) for i in range(N)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the IPython magic `%time` or `%timeit` to time the reading of data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tables.open_file('big.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(1000000,)) ''\n",
       "  description := {\n",
       "  \"i\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"momentum\": {\n",
       "  \"p_x\": Float64Col(shape=(), dflt=0.0, pos=0),\n",
       "  \"p_y\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"p_z\": Float64Col(shape=(), dflt=0.0, pos=2)}}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (4681,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.create_table('/', 'mydata', table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([(     0, (  0.00000000e+00,   0.00000000e+00,   0.00000000e+00)),\n",
       "       (     1, (  1.00000000e+00,   1.00000000e+00,   1.00000000e+00)),\n",
       "       (     2, (  2.00000000e+00,   2.00000000e+00,   2.00000000e+00)),\n",
       "       ...,\n",
       "       (999997, (  9.99997000e+05,   9.99997000e+05,   9.99997000e+05)),\n",
       "       (999998, (  9.99998000e+05,   9.99998000e+05,   9.99998000e+05)),\n",
       "       (999999, (  9.99999000e+05,   9.99999000e+05,   9.99999000e+05))],\n",
       "      dtype=[('i', '<i4'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time f.root.mydata[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time f.root.mydata.cols.momentum.p_x[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `h5py` there is no equivalent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File('big.h5', 'a')\n",
    "dset = f['mydata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time dset['momentum']['p_x'][20:30]  # this reads the entire nested columns and selects `p_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dset['momentum']['p_x'][20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Using the hierarchy and compound datasets (tables).\n",
    "\n",
    "`ufo-scrubbed.csv` is a (scrubbed) partial dataset of UFO Sightings from across the world.\n",
    "\n",
    "Store the UFO sightings in HDF5. Assume the real (full) dataset is VERY large. Store the data in multiple tables (geospatial).\n",
    "Make sure you use correct dtype etc.\n",
    "\n",
    "Use h5py and/or pytables.\n",
    "\n",
    "**The objective of this excercise is to get hands-on experience with handling compound datatypes, groups, dtypes etc.**\n",
    "\n",
    "-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, do not use `pandas` in this exercise (we will use it later on). But let's use pandas for a quick overview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "   duration (seconds) duration (hours/min)  \\\n",
       "0              2700.0           45 minutes   \n",
       "1              7200.0              1-2 hrs   \n",
       "2                20.0           20 seconds   \n",
       "3                20.0             1/2 hour   \n",
       "4               900.0           15 minutes   \n",
       "\n",
       "                                            comments date posted   latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.883056   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005  29.384210   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008  53.200000   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.978333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.418056   \n",
       "\n",
       "    longitude  \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/ufo_scrubbed.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset CSV and create a dict of sighting per country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('datasets/ufo_scrubbed.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    dataset = [tuple(line) for line in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datetime',\n",
       "  'city',\n",
       "  'state',\n",
       "  'country',\n",
       "  'shape',\n",
       "  'duration (seconds)',\n",
       "  'duration (hours/min)',\n",
       "  'comments',\n",
       "  'date posted',\n",
       "  'latitude',\n",
       "  'longitude'),\n",
       " ('10/10/1949 20:30',\n",
       "  'san marcos',\n",
       "  'tx',\n",
       "  'us',\n",
       "  'cylinder',\n",
       "  '2700',\n",
       "  '45 minutes',\n",
       "  'This event took place in early fall around 1949-50. It occurred after a Boy Scout meeting in the Baptist Church. The Baptist Church sit',\n",
       "  '4/27/2004',\n",
       "  '29.8830556',\n",
       "  '-97.9411111')]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictonary of sightings per country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "sightings = defaultdict(list)\n",
    "\n",
    "for sighting in dataset[1:]:\n",
    "    dt, city, state, country, _, duration, _, comments, _, lat, lon = sighting\n",
    "    sightings[country].append((dt, city, state, duration, lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['us', '', 'gb', 'ca', 'au', 'de'])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sightings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sightings['de'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list in the dictonary is a `numpy.recarray` like object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10/13/2006 00:02', 'berlin (germany)', '', '120', '52.516667', '13.4'),\n",
       " ('10/20/2012 18:00', 'berlin (germany)', '', '1500', '52.516667', '13.4'),\n",
       " ('10/8/2012 17:10', 'obernheim (germany)', '', '2', '49.366667', '7.583333')]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sightings['de'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually set the `dtype` (or use `tables.IsDescription`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype=np.dtype([('datetime', 'S16'), ('city', 'S20'), ('state', 'S2'), ('duration', np.float32), ('lat', 'f8'), ('lon', 'f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ (b'10/10/1949 20:30', b'san marcos', b'tx',  2700.,  29.8830556,  -97.9411111),\n",
       "       (b'10/10/1956 21:00', b'edna', b'tx',    20.,  28.9783333,  -96.6458333),\n",
       "       (b'10/10/1960 20:00', b'kaneohe', b'hi',   900.,  21.4180556, -157.8036111),\n",
       "       ...,\n",
       "       (b'9/9/2013 22:00', b'napa', b'ca',  1200.,  38.2972222, -122.2844444),\n",
       "       (b'9/9/2013 22:20', b'vienna', b'va',     5.,  38.9011111,  -77.2655556),\n",
       "       (b'9/9/2013 23:00', b'edmond', b'ok',  1020.,  35.6527778,  -97.4777778)],\n",
       "      dtype=[('datetime', 'S16'), ('city', 'S20'), ('state', 'S2'), ('duration', '<f4'), ('lat', '<f8'), ('lon', '<f8')])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sightings['us'], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(data_dir, 'ufo.h5')\n",
    "f = h5py.File(fn, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in sightings.keys():\n",
    "    if country=='':\n",
    "        group_name = 'other'\n",
    "    else:\n",
    "        group_name = country\n",
    "    f.create_group(group_name)\n",
    "    f.create_dataset('%s/data' % group_name, data=np.array(sightings[country], dtype=dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'ca', 'de', 'gb', 'other', 'us']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\n",
      "/au                      Group\n",
      "/au/data                 Dataset {538}\n",
      "/ca                      Group\n",
      "/ca/data                 Dataset {3000}\n",
      "/de                      Group\n",
      "/de/data                 Dataset {105}\n",
      "/gb                      Group\n",
      "/gb/data                 Dataset {1905}\n",
      "/other                   Group\n",
      "/other/data              Dataset {9670}\n",
      "/us                      Group\n",
      "/us/data                 Dataset {65114}\n"
     ]
    }
   ],
   "source": [
    "!h5ls -r {fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PyTables` datasets are wrapped into high levels objects:\n",
    "* **Array**: homogenous dataset\n",
    "* **CArray**: homogenous dataset, chunked storage (more on this later)\n",
    "* **EArray**: homogenous dataset, extendable. Supports `.append()`.\n",
    "* **Table**: compound dataset, extendable. Supports `.append()`.\n",
    "\n",
    "1) Create a hdf5 file in `PyTables` with each of the above for dataset types. Open it in `h5py` (and/or use `h5ls` etc). Note the equivalent `h5py` datatype.\n",
    "\n",
    "2) Create the same file in `h5py`, make sure the datasets end up as the above four `PyTables` dataset types. Use `ptdump` to view the file in `PyTables` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(data_dir, 'types.h5')\n",
    "with h5py.File(fn, 'w') as f:\n",
    "    # Array\n",
    "    f['/array'] = np.arange(10)\n",
    "    # CArray\n",
    "    dset = f.create_dataset('/carray', (10,), chunks=True)\n",
    "    dset[:] = np.arange(10)\n",
    "    # EArray\n",
    "    dset = f.create_dataset('/earray', (10,), maxshape=(None,))\n",
    "    dset[:] = np.arange(10)\n",
    "    dset.resize(20, 0)\n",
    "    dset[10:] = np.arange(10)\n",
    "    # Table\n",
    "    dtype=np.dtype([('c1', 'i4'), ('c2','f4')])\n",
    "    table = np.array([(1, 2.5), (3, 6.7)], dtype=dtype)\n",
    "    f['/table'] = table             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\n",
      "/array (Array(10,)) ''\n",
      "/carray (CArray(10,)) ''\n",
      "/earray (EArray(20,)) ''\n",
      "/table (Table(2,)) ''\n"
     ]
    }
   ],
   "source": [
    "!ptdump {fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hdf5]",
   "language": "python",
   "name": "conda-env-hdf5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
