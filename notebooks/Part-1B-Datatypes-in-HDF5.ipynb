{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1B: Datatypes in HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * How to create (and read) HDF5 files with datasets of homogeneous, heterogenous and nested datatypes\n",
    "> * See how h5py and PyTables achieves the same thing with their own APIs\n",
    "> * Be introduced to the `IsDescription` class in PyTables for declaring tables (instead of NumPy dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "data_dir = \"datatypes\"\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneous datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_to_store = np.arange(10, dtype=np.int8)\n",
    "arr_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"homogenous_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"mydata\": shape (10,), type \"|i1\">"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.create_dataset(data=arr_to_store, name=\"mydata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['/mydata2'] = arr_to_store    # data can be accessed in a NumPy-like interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mydata', 'mydata2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['/mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"datatypes\\homogenous_h5py.h5\" with sec2 driver.\n",
      "/                        Group\n",
      "    Location:  1:96\n",
      "    Links:     1\n",
      "/mydata                  Dataset {10/10}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\n",
      "    Type:      native signed char\n",
      "/mydata2                 Dataset {10/10}\n",
      "    Location:  1:1400\n",
      "    Links:     1\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\n",
      "    Type:      native signed char\n"
     ]
    }
   ],
   "source": [
    "!h5ls -rv {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 19 08:31 homogenous_h5py.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"homogenous_pytables.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Array(10,)) ''\n",
       "  atom := Int8Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.create_array(f2.root, name=\"mydata\", obj=arr_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.root.mydata[:]  # data can be accessed in a NumPy-like interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File(filename=datatypes\\homogenous_pytables.h5, title='', mode='w', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) ''\n",
       "/mydata (Array(10,)) ''\n",
       "  atom := Int8Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"datatypes\\homogenous_pytables.h5\" with sec2 driver.\n",
      "/                        Group\n",
      "    Attribute: CLASS scalar\n",
      "        Type:      5-byte null-terminated UTF-8 string\n",
      "        Data:  \"GROUP\"\n",
      "    Attribute: PYTABLES_FORMAT_VERSION scalar\n",
      "        Type:      3-byte null-terminated UTF-8 string\n",
      "        Data:  \"2.1\"\n",
      "    Attribute: TITLE null\n",
      "        Type:      1-byte null-terminated UTF-8 string\n",
      "\n",
      "    Attribute: VERSION scalar\n",
      "        Type:      3-byte null-terminated UTF-8 string\n",
      "        Data:  \"1.0\"\n",
      "    Location:  1:96\n",
      "    Links:     1\n",
      "/mydata                  Dataset {10/10}\n",
      "    Attribute: CLASS scalar\n",
      "        Type:      5-byte null-terminated UTF-8 string\n",
      "        Data:  \"ARRAY\"\n",
      "    Attribute: FLAVOR scalar\n",
      "        Type:      5-byte null-terminated UTF-8 string\n",
      "        Data:  \"numpy\"\n",
      "    Attribute: TITLE null\n",
      "        Type:      1-byte null-terminated UTF-8 string\n",
      "\n",
      "    Attribute: VERSION scalar\n",
      "        Type:      3-byte null-terminated UTF-8 string\n",
      "        Data:  \"2.4\"\n",
      "    Location:  1:1024\n",
      "    Links:     1\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\n",
      "    Type:      native signed char\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H5tools-DIAG: Error detected in HDF5:tools (1.8.17) thread 0:\n",
      "  #000: C:\\bld\\hdf5_1490811383644\\work\\hdf5-1.8.17\\tools\\lib\\h5tools_dump.c line 1836 in h5tools_dump_mem(): H5Sis_simple failed\n",
      "    major: Failure in tools library\n",
      "    minor: error in function\n",
      "  #001: C:\\bld\\hdf5_1490811383644\\work\\hdf5-1.8.17\\tools\\lib\\h5tools_dump.c line 1836 in h5tools_dump_mem(): H5Sis_simple failed\n",
      "    major: Failure in tools library\n",
      "    minor: error in function\n"
     ]
    }
   ],
   "source": [
    "!h5ls -rv {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 19 08:31 homogenous_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 2.2K Jun 19 08:31 homogenous_pytables.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compound Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = np.dtype([(\"myfield1\", np.int32), (\"myfield2\", np.float64), (\"myfield3\", \"S5\")])\n",
    "table_to_store = np.fromiter(((i, i**2, \"foo_%d\"%i) for i in range(10)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0, 0.0, b'foo_0'), (1, 1.0, b'foo_1'), (2, 4.0, b'foo_2'),\n",
       "       (3, 9.0, b'foo_3'), (4, 16.0, b'foo_4'), (5, 25.0, b'foo_5'),\n",
       "       (6, 36.0, b'foo_6'), (7, 49.0, b'foo_7'), (8, 64.0, b'foo_8'),\n",
       "       (9, 81.0, b'foo_9')], \n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['mydata'] = table_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"mydata\": shape (10,), type \"|V17\">"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0, 0.0, b'foo_0'), (1, 1.0, b'foo_1'), (2, 4.0, b'foo_2'),\n",
       "       (3, 9.0, b'foo_3'), (4, 16.0, b'foo_4'), (5, 25.0, b'foo_5'),\n",
       "       (6, 36.0, b'foo_6'), (7, 49.0, b'foo_7'), (8, 64.0, b'foo_8'),\n",
       "       (9, 81.0, b'foo_9')], \n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"datatypes\\compound_h5py.h5\" with sec2 driver.\n",
      "mydata                   Dataset {10/10}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Storage:   170 logical bytes, 170 allocated bytes, 100.00% utilization\n",
      "    Type:      struct {\n",
      "                   \"myfield1\"         +0    native int\n",
      "                   \"myfield2\"         +4    native double\n",
      "                   \"myfield3\"         +12   5-byte null-padded ASCII string\n",
      "               } 17 bytes\n"
     ]
    }
   ],
   "source": [
    "!h5ls -v {FILENAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `datatypes\\compound_h5py.h5` in PyTables and investigate the dataset.\n",
    "\n",
    "Look at the dataset description. Read it from disk. Look at the dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_h5py.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\n",
      "/mydata (Table(10,)) ''\n"
     ]
    }
   ],
   "source": [
    "!ptdump {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION STARTS HERE!!!\n",
    "fileh = tables.open_file(FILENAME, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File(filename=datatypes\\compound_h5py.h5, title='', mode='r', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) ''\n",
       "/mydata (Table(10,)) ''\n",
       "  description := {\n",
       "  \"myfield1\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"myfield2\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"myfield3\": StringCol(itemsize=5, shape=(), dflt=b'', pos=2)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3855,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0, 0.0, b'foo_0'), (1, 1.0, b'foo_1'), (2, 4.0, b'foo_2'),\n",
       "       (3, 9.0, b'foo_3'), (4, 16.0, b'foo_4'), (5, 25.0, b'foo_5'),\n",
       "       (6, 36.0, b'foo_6'), (7, 49.0, b'foo_7'), (8, 64.0, b'foo_8'),\n",
       "       (9, 81.0, b'foo_9')], \n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = fileh.root.mydata[:]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables (simple way)\n",
    "\n",
    "In PyTables a compound dataset is called a `Table`.\n",
    "\n",
    "To store a table we use `create_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_pytables1.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(0,)) ''\n",
       "  description := {\n",
       "  \"myfield1\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"myfield2\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"myfield3\": StringCol(itemsize=5, shape=(), dflt=b'', pos=2)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3855,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = f2.create_table(f2.root, name=\"mydata\", description=table_to_store.dtype)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Table` class has high level functions, such as `append()` and `read()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.append(table_to_store)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0, 0.0, b'foo_0'), (1, 1.0, b'foo_1'), (2, 4.0, b'foo_2'),\n",
       "       (3, 9.0, b'foo_3'), (4, 16.0, b'foo_4'), (5, 25.0, b'foo_5'),\n",
       "       (6, 36.0, b'foo_6'), (7, 49.0, b'foo_7'), (8, 64.0, b'foo_8'),\n",
       "       (9, 81.0, b'foo_9')], \n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.remove_row(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0, 0.0, b'foo_0'), (1, 1.0, b'foo_1'), (2, 4.0, b'foo_2'),\n",
       "       (3, 9.0, b'foo_3'), (4, 16.0, b'foo_4'), (6, 36.0, b'foo_6'),\n",
       "       (7, 49.0, b'foo_7'), (8, 64.0, b'foo_8'), (9, 81.0, b'foo_9')], \n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables (description way)\n",
    "\n",
    "In PyTables it is convenient to define compound datasets using the `tables.IsDescription` class, instead of (complicated) numpy dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyTable(tables.IsDescription):\n",
    "    myfield1 = tables.Int32Col()\n",
    "    myfield2 = tables.Float64Col()\n",
    "    myfield3 = tables.StringCol(itemsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_pytables2.h5\")\n",
    "f3 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = f3.create_table(f3.root, \"mydata\", MyTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.append(table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 156\n",
      "-rw-r--r-- 1 tomkooij 197613  2314 Jun 19 08:31 compound_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 69879 Jun 19 08:31 compound_pytables1.h5\n",
      "-rw-r--r-- 1 tomkooij 197613 69879 Jun 19 08:31 compound_pytables2.h5\n",
      "-rw-r--r-- 1 tomkooij 197613  2164 Jun 19 08:31 homogenous_h5py.h5\n",
      "-rw-r--r-- 1 tomkooij 197613  2154 Jun 19 08:31 homogenous_pytables.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -l {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it seems like PyTables files are larger than h5py ones, why?  Let's introspect a bit into the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {10}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {data_dir}/compound_h5py.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {9/Inf}\n"
     ]
    }
   ],
   "source": [
    "!h5ls {data_dir}/compound_pytables1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dimensionality of the table created with PyTables is `{10/Inf}`, indicating that the dataset is chunked, whereas the one created with h5py is just `{10}`, which means that it is not using chunking.  As chunked datasets take more space than non-chunked ones, this is why PyTables are larger.\n",
    "\n",
    "The reason why PyTables tables are chunked by default is that they can be enlarged and compressed, and chunking is required in order to allow that.  More on chunking later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nested fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NestedTable(tables.IsDescription):\n",
    "    \"\"\"A nested table\"\"\"\n",
    "    name = tables.StringCol(10, pos=0)\n",
    "    \n",
    "    class momentum(tables.IsDescription):\n",
    "        p_x = tables.Float64Col()\n",
    "        p_y = tables.Float64Col()\n",
    "        p_z = tables.Float64Col() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"nested.h5\")\n",
    "f4 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = f4.create_table(f4.root, \"mydata\", NestedTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(0,)) ''\n",
       "  description := {\n",
       "  \"name\": StringCol(itemsize=10, shape=(), dflt=b'', pos=0),\n",
       "  \"momentum\": {\n",
       "    \"p_x\": Float64Col(shape=(), dflt=0.0, pos=0),\n",
       "    \"p_y\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "    \"p_z\": Float64Col(shape=(), dflt=0.0, pos=2)}}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1927,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = t.dtype\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'foo_0', (0.0, 10.0, 20.0)), (b'foo_1', (1.0, 11.0, 21.0)),\n",
       "       (b'foo_2', (2.0, 12.0, 22.0)), (b'foo_3', (3.0, 13.0, 23.0)),\n",
       "       (b'foo_4', (4.0, 14.0, 24.0)), (b'foo_5', (5.0, 15.0, 25.0)),\n",
       "       (b'foo_6', (6.0, 16.0, 26.0)), (b'foo_7', (7.0, 17.0, 27.0)),\n",
       "       (b'foo_8', (8.0, 18.0, 28.0)), (b'foo_9', (9.0, 19.0, 29.0))], \n",
       "      dtype=[('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store = np.fromiter(((\"foo_%s\"%i, (i, 10+i, 20+i)) for i in range(10)), dtype=dtype)\n",
    "table_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store['momentum']['p_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.append(table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'foo_0', (0.0, 10.0, 20.0)), (b'foo_1', (1.0, 11.0, 21.0)),\n",
       "       (b'foo_2', (2.0, 12.0, 22.0)), (b'foo_3', (3.0, 13.0, 23.0)),\n",
       "       (b'foo_4', (4.0, 14.0, 24.0)), (b'foo_5', (5.0, 15.0, 25.0)),\n",
       "       (b'foo_6', (6.0, 16.0, 26.0)), (b'foo_7', (7.0, 17.0, 27.0)),\n",
       "       (b'foo_8', (8.0, 18.0, 28.0)), (b'foo_9', (9.0, 19.0, 29.0))], \n",
       "      dtype=[('name', 'S10'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `Cols` accessor. (PyTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.col(name)` reads the entire column.\n",
    "\n",
    "`table.col('momentum')` will read the entire column (array) in memory and slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(2.0, 12.0, 22.0), (3.0, 13.0, 23.0), (4.0, 14.0, 24.0)], \n",
       "      dtype=[('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.col('momentum')[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `cols` accessor, we can access the column without reading the entire column in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata.cols.momentum (Cols), 3 columns\n",
       "  p_x (Column(10,), float64)\n",
       "  p_y (Column(10,), float64)\n",
       "  p_z (Column(10,), float64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cols.momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(2.0, 12.0, 22.0), (3.0, 13.0, 23.0), (4.0, 14.0, 24.0)], \n",
       "      dtype=[('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cols.momentum[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested columns can be accessed by the `Cols` accessor using natural naming: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata.cols.momentum.p_x (Column(10,), float64, idx=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cols.momentum.p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Investigate reading a small part of a large table from disk.\n",
    "\n",
    " * Store the table in a HDF5 file. (Using either PyTables or h5py).\n",
    " * Read elements [20:30] from the p_x column.\n",
    "\n",
    "Is the entire datafile being read from disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NestedTable(tables.IsDescription):\n",
    "    \"\"\"A nested table\"\"\"\n",
    "    i = tables.Int32Col()\n",
    "    \n",
    "    class momentum(tables.IsDescription):\n",
    "        p_x = tables.Float64Col()\n",
    "        p_y = tables.Float64Col()\n",
    "        p_z = tables.Float64Col() \n",
    "        \n",
    "dtype = tables.description.dtype_from_descr(NestedTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = int(1e6)\n",
    "table_to_store = np.fromiter(((i, (i, i, i)) for i in range(N)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION STARTS HERE\n",
    "f = tables.open_file('big.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(1000000,)) ''\n",
       "  description := {\n",
       "  \"i\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"momentum\": {\n",
       "  \"p_x\": Float64Col(shape=(), dflt=0.0, pos=0),\n",
       "  \"p_y\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"p_z\": Float64Col(shape=(), dflt=0.0, pos=2)}}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (4681,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.create_table('/', 'mydata', table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([(0, (0.0, 0.0, 0.0)), (1, (1.0, 1.0, 1.0)), (2, (2.0, 2.0, 2.0)),\n",
       "       ..., (999997, (999997.0, 999997.0, 999997.0)),\n",
       "       (999998, (999998.0, 999998.0, 999998.0)),\n",
       "       (999999, (999999.0, 999999.0, 999999.0))], \n",
       "      dtype=[('i', '<i4'), ('momentum', [('p_x', '<f8'), ('p_y', '<f8'), ('p_z', '<f8')])])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time f.root.mydata[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time f.root.mydata.cols.momentum.p_x[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `h5py` there is no equivalent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File('big.h5', 'a')\n",
    "dset = f['mydata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 75 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   1.00000000e+00,   2.00000000e+00, ...,\n",
       "         9.99997000e+05,   9.99998000e+05,   9.99999000e+05])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dset['momentum']['p_x']  # this reads the entire nested columns and selects `p_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Using the hierarchy and compound datasets (tables).\n",
    "\n",
    "`ufo-scrubbed.csv` is a (scrubbed) partial dataset of UFO Sightings from across the world.\n",
    "\n",
    "Store the UFO sightings in HDF5. Assume the real (full) dataset is VERY large. Store the data in multiple tables (geospatial).\n",
    "Make sure you use correct dtype etc.\n",
    "\n",
    "Use h5py and/or pytables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/ufo_scrubbed.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    dataset = [tuple(line) for line in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datetime',\n",
       "  'city',\n",
       "  'state',\n",
       "  'country',\n",
       "  'shape',\n",
       "  'duration (seconds)',\n",
       "  'duration (hours/min)',\n",
       "  'comments',\n",
       "  'date posted',\n",
       "  'latitude',\n",
       "  'longitude '),\n",
       " ('10/10/1949 20:30',\n",
       "  'san marcos',\n",
       "  'tx',\n",
       "  'us',\n",
       "  'cylinder',\n",
       "  '2700',\n",
       "  '45 minutes',\n",
       "  'This event took place in early fall around 1949-50. It occurred after a Boy Scout meeting in the Baptist Church. The Baptist Church sit',\n",
       "  '4/27/2004',\n",
       "  '29.8830556',\n",
       "  '-97.9411111')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictonary of sightings per country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "sightings = defaultdict(list)\n",
    "\n",
    "for sighting in dataset[1:]:\n",
    "    dt, city, state, country, _, duration, _, comments, _, lat, lon = sighting\n",
    "    sightings[country].append((dt, city, state, duration, lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['us', '', 'gb', 'ca', 'au', 'de'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sightings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sightings['de'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# SOLUTION STARTS HERE\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype=np.dtype([('datetime', 'S16'), ('city', 'S20'), ('state', 'S2'), ('duration', np.int32), ('lat', 'f8'), ('lon', 'f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'10/13/2006 00:02', b'berlin (germany)', b'', 120, 52.516667, 13.4),\n",
       "       (b'10/20/2012 18:00', b'berlin (germany)', b'', 1500, 52.516667, 13.4),\n",
       "       (b'10/8/2012 17:10', b'obernheim (germany)', b'', 2, 49.366667, 7.583333),\n",
       "       (b'1/10/2011 18:38', b'ottersberg (germany)', b'', 240, 53.1, 9.15),\n",
       "       (b'11/15/1990 22:30', b'bremen (30 km south ', b'', 30, 50.716667, 10.0),\n",
       "       (b'11/15/2005 15:00', b'sembach (germany)', b'', 120, 49.516667, 7.85),\n",
       "       (b'11/18/2002 16:35', b'magdeburg (germany)', b'', 4, 52.166667, 11.666667),\n",
       "       (b'1/1/2008 22:30', b'neuruppin (germany)', b'', 900, 52.933333, 12.8),\n",
       "       (b'1/1/2009 00:00', b'lampertheim (germany', b'', 1560, 49.601944, 8.471944),\n",
       "       (b'1/1/2009 00:15', b'ramstein (germany)', b'', 7200, 49.45, 7.533333),\n",
       "       (b'12/10/1987 21:00', b'nurenburg (germany)', b'', 28800, 49.447778, 11.068333),\n",
       "       (b'12/15/1998 22:50', b'senftenberg (germany', b'', 60, 51.516667, 14.016667),\n",
       "       (b'12/18/2003 20:00', b'schwalmtal (germany)', b'', 480, 51.216667, 6.266667),\n",
       "       (b'12/21/2008 20:31', b'neuss (germany)', b'', 1800, 51.2, 6.683333),\n",
       "       (b'1/22/2004 06:25', b'babenhausen (germany', b'', 2, 52.05, 8.483333),\n",
       "       (b'12/24/2001 04:17', b'berlin (germany)', b'', 60, 52.516667, 13.4),\n",
       "       (b'12/26/1996 15:00', b'mittenwald (germany)', b'', 900, 47.433333, 11.25),\n",
       "       (b'12/26/2007 13:25', b'ransbach-baumbach (g', b'', 12, 50.466667, 7.733333),\n",
       "       (b'1/23/2001 15:30', b'ansbach (germany)', b'', 240, 49.3, 10.583333),\n",
       "       (b'1/24/2012 07:50', b'miesau (germany)', b'', 2, 49.4, 7.433333),\n",
       "       (b'1/25/2003 14:03', b'bensheim (germany)', b'', 15, 49.680278, 8.616111),\n",
       "       (b'12/6/2003 13:00', b'muenster (germany)', b'', 300, 51.966667, 7.633333),\n",
       "       (b'1/30/1994 18:00', b'chemnitz (germany)', b'', 20, 50.833333, 12.916667),\n",
       "       (b'1/30/2004 23:00', b'kirchzell (germany)', b'', 1200, 49.618889, 9.177778),\n",
       "       (b'1/6/2014 00:00', b'bremen (germany)', b'', 10800, 50.716667, 10.0),\n",
       "       (b'2/12/1969 23:30', b'wildflecken (germany', b'', 300, 50.366667, 9.916667),\n",
       "       (b'2/15/1991 00:00', b'munich (germany)', b'', 120, 48.15, 11.5833),\n",
       "       (b'2/15/1992 23:30', b'baumholder (germany)', b'', 1800, 49.616667, 7.333333),\n",
       "       (b'2/27/1975 06:25', b'zirndorf (west germa', b'', 3, 49.45, 10.95),\n",
       "       (b'2/4/2003 23:15', b'hamburg (germany)', b'', 18000, 53.55, 10.0),\n",
       "       (b'3/12/2005 14:30', b'langenleiten (german', b'', 180, 50.333333, 9.983333),\n",
       "       (b'3/15/1962 12:00', b'baumholder (germany)', b'', 180, 49.616667, 7.333333),\n",
       "       (b'3/17/2012 19:15', b'zehdenick (germany)', b'', 180, 52.983333, 13.333333),\n",
       "       (b'3/2/2006 22:00', b'hanau (germany)', b'', 600, 50.133333, 8.916667),\n",
       "       (b'3/23/2009 22:07', b'berlin (germany)', b'', 300, 52.516667, 13.4),\n",
       "       (b'3/27/2004 22:25', b'aachen (near cologne', b'', 2, 50.770833, 6.105278),\n",
       "       (b'4/10/1968 17:00', b'munich (oberschliess', b'', 600, 48.15, 11.5833),\n",
       "       (b'4/15/2013 24:00', b'munich (near) (germa', b'', 120, 48.15, 11.5833),\n",
       "       (b'4/16/2007 21:14', b'bremen (germany)', b'', 60, 50.716667, 10.0),\n",
       "       (b'4/17/2010 21:00', b'berlin (germany)', b'', 240, 52.516667, 13.4),\n",
       "       (b'4/21/1990 22:10', b'bad pyrmont (germany', b'', 20, 51.983333, 9.25),\n",
       "       (b'4/25/2012 01:00', b'freiburg (germany)', b'', 40, 50.916667, 13.366667),\n",
       "       (b'4/7/2014 22:00', b'frankfurt am main (g', b'', 120, 50.116667, 8.683333),\n",
       "       (b'5/10/2002 01:00', b'siegen (germany)', b'', 1200, 50.866667, 8.033333),\n",
       "       (b'5/10/2008 12:00', b'erlangen (germany)', b'', 1200, 49.589722, 11.003889),\n",
       "       (b'5/13/2000 22:45', b'osnabruck (germany)', b'', 300, 52.266667, 8.05),\n",
       "       (b'5/14/2007 01:25', b'kelsterbach (germany', b'', 150, 50.066667, 8.533333),\n",
       "       (b'5/15/1971 02:00', b'trier (germany)', b'', 15, 49.75, 6.633333),\n",
       "       (b'5/29/2003 23:45', b'elbingen (germany)', b'', 60, 50.533333, 7.916667),\n",
       "       (b'5/6/2014 24:00', b'bocholt (germany)', b'', 240, 51.833333, 6.6),\n",
       "       (b'5/7/1995 18:00', b'emmelshausen (german', b'', 7200, 50.15, 7.566667),\n",
       "       (b'6/1/1968 05:00', b'darmstadt (germany)', b'', 21600, 49.870556, 8.649444),\n",
       "       (b'6/13/1981 22:00', b'stuttgart (germany)', b'', 300, 48.766667, 9.183333),\n",
       "       (b'6/14/2013 23:01', b'berlin (germany)', b'', 30, 52.516667, 13.4),\n",
       "       (b'6/15/1996 02:00', b'ansbach (germany)', b'', 5, 49.3, 10.583333),\n",
       "       (b'6/15/2005 14:32', b'frankfurt (germany)', b'', 300, 52.35, 14.55),\n",
       "       (b'6/17/2008 22:00', b'dresden (germany)', b'', 180, 51.05, 13.75),\n",
       "       (b'6/21/2007 23:10', b'mainz (germany)', b'', 4, 50.0, 8.271111),\n",
       "       (b'6/21/2013 19:30', b'werder (havel) (germ', b'', 3, 52.133333, 11.65),\n",
       "       (b'6/24/1974 14:40', b'schweinfurt (west ge', b'', 120, 50.05, 10.233333),\n",
       "       (b'6/26/2006 04:30', b'emlichheim (germany)', b'', 10, 52.616667, 6.85),\n",
       "       (b'6/28/2009 00:24', b'staufen (germany)', b'', 900, 48.666667, 10.283333),\n",
       "       (b'6/30/2008 03:00', b'neuseddin (potsdam)(', b'', 1814400, 52.283333, 12.983333),\n",
       "       (b'6/6/1970 18:00', b'mannheim (west germa', b'', 30, 49.488333, 8.464722),\n",
       "       (b'6/6/2004 11:50', b'schafhausen (germany', b'', 120, 50.566667, 10.183333),\n",
       "       (b'6/6/2008 15:00', b'berlin (germany)', b'', 180, 52.516667, 13.4),\n",
       "       (b'6/7/2008 23:00', b'erfurt (thuringia&#4', b'', 1200, 50.983333, 11.033333),\n",
       "       (b'6/8/2010 19:45', b'munich (germany)', b'', 120, 48.15, 11.5833),\n",
       "       (b'7/1/1979 13:00', b'waldorf (west german', b'', 300, 50.483333, 7.233333),\n",
       "       (b'7/14/2001 23:15', b'bamberg (germany/bav', b'', 180, 49.866667, 10.866667),\n",
       "       (b'7/15/2006 18:00', b'fulda (near) (german', b'', 2, 50.55, 9.666667),\n",
       "       (b'7/15/2007 00:45', b'ansbach (germany)', b'', 900, 49.3, 10.583333),\n",
       "       (b'7/17/2008 22:00', b'dresden (germany)', b'', 180, 51.05, 13.75),\n",
       "       (b'7/18/1987 12:30', b'bierenbachtal (germa', b'', 600, 50.916667, 7.55),\n",
       "       (b'7/18/2004 23:45', b'kassel (germany) (on', b'', 180, 51.316667, 9.5),\n",
       "       (b'7/21/2003 02:30', b'bamberg (germany)', b'', 604800, 49.866667, 10.866667),\n",
       "       (b'7/22/2002 15:00', b'maugenhard (germany)', b'', 180, 47.67489, 7.631976),\n",
       "       (b'7/25/2008 15:04', b'regensburg (germany)', b'', 2, 49.015, 12.095556),\n",
       "       (b'7/25/2008 22:00', b'berlin (germany)', b'', 60, 52.516667, 13.4),\n",
       "       (b'7/28/2012 21:45', b'berlin (germany)', b'', 300, 52.516667, 13.4),\n",
       "       (b'7/30/2008 01:30', b'ramstein (germany)', b'', 3600, 49.45, 7.533333),\n",
       "       (b'7/3/1997 23:00', b'bochum (germany)', b'', 120, 51.483333, 7.216667),\n",
       "       (b'7/3/2007 02:30', b'mainz (germany)', b'', 7, 50.0, 8.271111),\n",
       "       (b'8/1/1969 20:00', b'berlin (germany)', b'', 300, 52.516667, 13.4),\n",
       "       (b'8/1/2007 04:00', b'neumarkt (germany)', b'', 5, 51.35, 12.016667),\n",
       "       (b'8/1/2010 24:00', b'munich (germany)', b'', 30, 48.15, 11.5833),\n",
       "       (b'8/13/2003 22:05', b'biesenthal (germany)', b'', 2, 52.75, 11.566667),\n",
       "       (b'8/15/2013 15:30', b'haus (germany)', b'', 5, 47.583333, 9.866667),\n",
       "       (b'8/15/2013 20:31', b'freiburg (germany)', b'', 60, 50.916667, 13.366667),\n",
       "       (b'8/17/2013 20:50', b'obernheim (germany)', b'', 600, 49.366667, 7.583333),\n",
       "       (b'8/20/2000 23:00', b'weissenburg (germany', b'', 90, 50.733333, 11.45),\n",
       "       (b'8/23/2008 22:30', b'bitburg (germany)', b'', 1800, 49.966667, 6.533333),\n",
       "       (b'8/23/2008 22:55', b'berlin (germany)', b'', 15, 52.516667, 13.4),\n",
       "       (b'8/26/2008 21:00', b'heidelberg (germany)', b'', 300, 50.633333, 13.466667),\n",
       "       (b'8/29/2000 01:30', b'hannover (germany)', b'', 900, 53.183333, 8.516667),\n",
       "       (b'8/3/1973 22:30', b'schwetzingen (german', b'', 60, 49.378056, 8.581944),\n",
       "       (b'8/9/2008 22:50', b'buchholz (germany)', b'', 60, 52.6, 13.433333),\n",
       "       (b'8/9/2010 00:30', b'cologne (germany)', b'', 180, 50.933333, 6.95),\n",
       "       (b'9/10/1999 21:10', b'weiden (ne bavaria) ', b'', 6, 51.066667, 11.366667),\n",
       "       (b'9/12/2009 19:00', b'grafenhausen (german', b'', 60, 49.233333, 7.966667),\n",
       "       (b'9/13/2011 12:00', b'heilbronn (germany)', b'', 5, 49.166667, 10.366667),\n",
       "       (b'9/16/2007 08:15', b'gelsenkirchen (germa', b'', 20, 51.516667, 7.05),\n",
       "       (b'9/16/2007 18:15', b'neckarsulm (germany)', b'', 30, 49.192778, 9.226111),\n",
       "       (b'9/4/2011 05:00', b'mannheim (germany)', b'', 1800, 49.488333, 8.464722),\n",
       "       (b'9/9/2009 21:38', b'kaiserlautern (germa', b'', 40, 49.45, 7.75)], \n",
       "      dtype=[('datetime', 'S16'), ('city', 'S20'), ('state', 'S2'), ('duration', '<i4'), ('lat', '<f8'), ('lon', '<f8')])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sightings['de'], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
